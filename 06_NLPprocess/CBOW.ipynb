{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2baf01fc210>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['We', 'are', 'to', 'study'], 'about'), (['are', 'about', 'study', 'the'], 'to'), (['about', 'to', 'the', 'idea'], 'study'), (['to', 'study', 'idea', 'of'], 'the'), (['study', 'the', 'of', 'a'], 'idea')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "raw_text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules\n",
    "called a program. People create programs to direct processes. In effect,\n",
    "we conjure the spirits of the computer with our spells.\"\"\".split()\n",
    "\n",
    "# By deriving a set from `raw_text`, we deduplicate the array\n",
    "vocab = set(raw_text)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "data = []\n",
    "for i in range(2, len(raw_text) - 2):\n",
    "    context = [raw_text[i - 2], raw_text[i - 1],\n",
    "               raw_text[i + 1], raw_text[i + 2]]\n",
    "    target = raw_text[i]\n",
    "    data.append((context, target))\n",
    "print(data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(2*context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    def test_embedding(self,inputs):\n",
    "        if inputs.data[0] == 36:\n",
    "            print(self.embeddings(inputs[0]))\n",
    "            print(self.embeddings.weight)\n",
    "        else:\n",
    "            pass\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        self.test_embedding(inputs)\n",
    "        \n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs\n",
    "    \n",
    "\n",
    "# create your model and train.  here are some functions to help you make\n",
    "# the data ready for use by your module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, 10, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    #print(tensor)\n",
    "    return autograd.Variable(tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0*******\n",
      "['are', 'about', 'study', 'the']\n",
      "Variable containing:\n",
      "-0.2805  0.8897  1.1810 -0.0653  0.7099  1.4811  1.4208  1.2979 -0.0694 -0.3817\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.7747 -0.7813 -0.8984  1.2846 -0.7147  2.1725 -0.9450 -0.2979 -0.0682  0.0382\n",
      "-0.4303 -2.7833  2.4552  0.0798  0.2870 -0.5868 -1.2120 -0.1455  1.1880 -2.0991\n",
      " 0.0692 -1.0046 -1.4106 -0.0915 -1.2086 -0.7630 -0.0951 -0.4809  0.6724 -0.9945\n",
      "-1.2721 -0.8173 -0.1642 -0.4784 -0.0912 -2.4624  0.9267  0.9246  0.0243 -0.3899\n",
      "-0.2931 -0.2631  1.9666  0.0820 -0.7569  0.4529  0.3982  0.8903  0.7367  0.7852\n",
      "-2.0423  0.6389 -0.5267 -1.1582 -0.1310 -0.9850  0.5880 -0.7275 -1.1458  1.2855\n",
      " 0.1970 -0.2333 -2.5346 -1.2132  0.2244 -1.5374  1.9983 -0.0735  0.2364 -0.8533\n",
      " 0.1642 -0.2454 -1.3531 -1.7252 -1.1503 -2.7380  0.7589 -0.0798  0.3698  0.0653\n",
      " 0.8205  1.4976 -1.5908  0.3253 -0.2204 -0.6457  1.4039 -0.4400 -0.7118 -0.4682\n",
      "-0.8177 -0.7243  1.1279  0.7613  0.5012  1.1098  0.1932 -2.0337 -0.7202  0.0267\n",
      " 0.8882  0.6051  2.4186 -0.7303 -1.2679 -0.1943 -1.7197  0.0886 -1.0088 -0.4804\n",
      "-0.8480  1.4590  0.2580  0.1403 -2.7082 -1.3364  0.4826 -0.2771  1.1252  0.2093\n",
      " 1.2578 -0.4974  1.5943  2.4811 -0.7333  1.2039  0.0595 -0.0303 -1.6371  0.3179\n",
      "-2.1192 -1.2336  1.5152 -0.0651  1.5388 -0.6897 -0.5030  0.5014 -0.5549  1.2586\n",
      " 0.0998  0.5918 -0.4684  0.2659  0.3243  0.3714 -0.3414  0.1161 -0.7829  0.4069\n",
      " 0.2276 -0.2330  1.9982  0.8340  0.8643  1.5306 -0.1057  0.6405  0.4193 -0.6407\n",
      " 0.5246 -0.1530  0.3435  0.9095 -0.8602  1.6048  0.2953  0.6755  0.0140 -0.3626\n",
      "-0.3361  0.4680  0.7140  1.8719 -0.9728 -0.9812 -0.3395 -1.4955  0.8483  0.9598\n",
      " 0.7000  0.5579 -1.2331  0.8949  0.5487  1.8287  0.0657 -0.9747  2.8342  1.3817\n",
      "-0.3728  0.1020  1.2787 -1.0686 -0.6934 -0.6946 -1.0273 -1.1994  1.1341  0.1492\n",
      " 0.9135  0.5589 -2.1980 -0.6292 -0.5239 -0.2811 -0.8155  0.0742 -0.3170 -0.7722\n",
      " 2.2769  0.2801  2.2227 -0.1356 -1.4328  0.5368  0.7172 -2.2966  0.5539 -0.8976\n",
      " 1.1879  0.6137 -1.0878  0.6980  0.9438 -1.6560  1.3341  1.8666 -0.0912 -1.1304\n",
      "-1.0469 -1.3789  0.2734  0.1551 -0.9394 -0.4639  0.0390  0.9946 -0.5208  0.4149\n",
      " 0.8287  1.3303  1.1712 -2.4700  0.3780 -2.0351  0.5423 -1.5329  0.0624  1.6044\n",
      "-0.7979  0.7888  1.1353 -0.2923 -1.8246 -0.2963  0.9344  0.8448  0.2216  0.5938\n",
      " 0.2049 -0.1363 -0.6238  0.3782 -0.9423  0.3295  1.1560  0.6051  0.2797 -0.5116\n",
      "-0.9022 -0.6203 -1.9109  0.8310  0.4975  0.2945 -0.4005  0.7591 -0.3009  0.9994\n",
      "-0.4797  1.3067  0.5923 -1.1249  1.2718 -1.0595 -0.4104 -0.6534 -0.8144 -1.6652\n",
      " 0.4518 -0.7686  1.0661 -0.4458  0.3354  0.7510 -0.7204 -0.0033  0.6147 -0.7341\n",
      "-0.7143  1.7093  0.1960 -0.5253 -0.0526 -2.2967 -0.4031 -0.0243  0.0236 -0.5117\n",
      "-2.3863 -0.7984  0.6166  0.9656  1.2048 -0.3057  1.3841 -0.4625 -0.2639 -1.3832\n",
      "-0.3717 -1.3293 -0.9960  0.5958  0.7258  0.9110  1.1518  0.5555  1.2097  0.5918\n",
      " 0.8163  0.6649 -1.0527  1.0583  0.1165 -1.3510  0.4548  0.2701 -1.9095  1.1432\n",
      "-1.4910  0.4840 -0.0215  1.1238  0.3548  1.3593  0.4050  0.0623 -0.3580  0.0351\n",
      " 0.5051 -0.3127  0.8936  0.3954 -0.6543  0.8657 -0.1654  0.9592  0.8287  1.1953\n",
      "-0.2805  0.8897  1.1810 -0.0653  0.7099  1.4811  1.4208  1.2979 -0.0694 -0.3817\n",
      " 0.5933 -0.0881 -0.1066 -0.4257  0.0075 -0.2048 -1.7848 -0.3742 -1.0483 -0.7345\n",
      "-0.6016 -1.8175  0.1243 -1.2455 -0.8114 -0.7161  0.5977 -0.6987  0.0524 -0.8667\n",
      " 1.3517 -0.0251  0.4184  0.0143  0.9916 -1.2906  0.0947  0.4591 -0.6553  0.2089\n",
      " 0.3357  0.0520 -0.3403  1.3421  0.3169 -0.1164  0.7450 -0.8583  0.3657 -0.0178\n",
      " 0.7084  0.1431  2.1606  0.0012  0.5635  0.8140  1.0198  0.1802  0.1627 -1.9319\n",
      "-0.8234 -1.0082  0.4692 -1.0969 -0.3684 -1.2028  0.6203 -1.6605  0.6156  0.6188\n",
      " 0.7899 -0.8541 -0.4620 -1.9286  0.0398 -0.5211 -0.6014 -0.2853  0.8449 -0.0928\n",
      "-0.5510 -0.0881 -0.5869  0.4249 -0.4805 -0.5812  0.2351  1.4621 -0.2184  0.5115\n",
      "-1.6912  0.2060 -0.5324 -1.2416 -1.3821  0.0059 -0.2811  0.1531  0.0302  1.9377\n",
      " 1.3922 -1.5565  1.0860 -1.1586 -0.0819  0.9277  1.7415 -1.2266 -2.1864 -0.2378\n",
      " 0.2321  0.2196 -0.6305  0.0018  1.5187  0.6556  1.2813 -0.0524  1.0704 -1.0734\n",
      " 0.9185  0.4504  0.7208 -1.3434  0.6324  0.4745 -0.7284  1.7754  0.4718  0.7148\n",
      "[torch.FloatTensor of size 49x10]\n",
      "\n",
      "['are', 'abstract', 'that', 'inhabit']\n",
      "Variable containing:\n",
      "-0.2803  0.8897  1.1810 -0.0653  0.7099  1.4811  1.4208  1.2979 -0.0694 -0.3817\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.7747 -0.7813 -0.8984  1.2846 -0.7147  2.1725 -0.9450 -0.2979 -0.0682  0.0382\n",
      "-0.4303 -2.7833  2.4552  0.0798  0.2870 -0.5868 -1.2120 -0.1455  1.1880 -2.0991\n",
      " 0.0692 -1.0046 -1.4106 -0.0915 -1.2086 -0.7630 -0.0951 -0.4809  0.6724 -0.9945\n",
      "-1.2721 -0.8173 -0.1642 -0.4784 -0.0912 -2.4624  0.9267  0.9246  0.0243 -0.3899\n",
      "-0.2931 -0.2631  1.9666  0.0820 -0.7569  0.4529  0.3982  0.8903  0.7367  0.7852\n",
      "-2.0424  0.6390 -0.5266 -1.1582 -0.1311 -0.9849  0.5880 -0.7274 -1.1458  1.2856\n",
      " 0.1970 -0.2333 -2.5346 -1.2132  0.2244 -1.5374  1.9983 -0.0735  0.2364 -0.8533\n",
      " 0.1642 -0.2454 -1.3531 -1.7252 -1.1503 -2.7380  0.7589 -0.0798  0.3698  0.0653\n",
      " 0.8205  1.4975 -1.5907  0.3253 -0.2206 -0.6457  1.4039 -0.4399 -0.7118 -0.4683\n",
      "-0.8177 -0.7243  1.1279  0.7613  0.5012  1.1098  0.1932 -2.0337 -0.7202  0.0267\n",
      " 0.8882  0.6051  2.4186 -0.7303 -1.2679 -0.1943 -1.7197  0.0886 -1.0088 -0.4804\n",
      "-0.8480  1.4590  0.2580  0.1403 -2.7082 -1.3364  0.4826 -0.2771  1.1252  0.2093\n",
      " 1.2578 -0.4973  1.5943  2.4811 -0.7333  1.2039  0.0594 -0.0303 -1.6371  0.3178\n",
      "-2.1192 -1.2335  1.5152 -0.0651  1.5387 -0.6896 -0.5029  0.5015 -0.5548  1.2587\n",
      " 0.0997  0.5918 -0.4684  0.2659  0.3244  0.3715 -0.3414  0.1162 -0.7829  0.4069\n",
      " 0.2276 -0.2330  1.9982  0.8340  0.8643  1.5306 -0.1057  0.6405  0.4193 -0.6407\n",
      " 0.5246 -0.1530  0.3435  0.9095 -0.8602  1.6048  0.2953  0.6755  0.0140 -0.3626\n",
      "-0.3361  0.4680  0.7139  1.8719 -0.9726 -0.9813 -0.3395 -1.4956  0.8483  0.9597\n",
      " 0.7000  0.5579 -1.2331  0.8949  0.5487  1.8287  0.0657 -0.9747  2.8342  1.3817\n",
      "-0.3728  0.1020  1.2787 -1.0686 -0.6934 -0.6948 -1.0273 -1.1994  1.1340  0.1491\n",
      " 0.9135  0.5590 -2.1980 -0.6292 -0.5238 -0.2812 -0.8155  0.0742 -0.3170 -0.7722\n",
      " 2.2769  0.2801  2.2227 -0.1356 -1.4328  0.5368  0.7172 -2.2966  0.5539 -0.8976\n",
      " 1.1879  0.6137 -1.0878  0.6980  0.9438 -1.6560  1.3341  1.8666 -0.0912 -1.1304\n",
      "-1.0469 -1.3789  0.2734  0.1551 -0.9394 -0.4639  0.0390  0.9946 -0.5208  0.4149\n",
      " 0.8287  1.3303  1.1712 -2.4700  0.3780 -2.0351  0.5423 -1.5329  0.0624  1.6044\n",
      "-0.7979  0.7887  1.1353 -0.2923 -1.8245 -0.2963  0.9343  0.8448  0.2216  0.5938\n",
      " 0.2049 -0.1363 -0.6238  0.3782 -0.9423  0.3295  1.1560  0.6051  0.2797 -0.5116\n",
      "-0.9022 -0.6203 -1.9109  0.8310  0.4975  0.2945 -0.4005  0.7591 -0.3009  0.9994\n",
      "-0.4797  1.3067  0.5923 -1.1249  1.2718 -1.0595 -0.4104 -0.6534 -0.8144 -1.6652\n",
      " 0.4518 -0.7686  1.0660 -0.4456  0.3354  0.7510 -0.7204 -0.0033  0.6146 -0.7342\n",
      "-0.7143  1.7093  0.1960 -0.5253 -0.0526 -2.2967 -0.4031 -0.0243  0.0236 -0.5117\n",
      "-2.3863 -0.7984  0.6166  0.9656  1.2048 -0.3057  1.3841 -0.4625 -0.2639 -1.3832\n",
      "-0.3717 -1.3293 -0.9960  0.5958  0.7258  0.9110  1.1518  0.5555  1.2097  0.5918\n",
      " 0.8163  0.6650 -1.0527  1.0583  0.1164 -1.3509  0.4547  0.2702 -1.9094  1.1431\n",
      "-1.4911  0.4840 -0.0216  1.1238  0.3548  1.3593  0.4050  0.0623 -0.3579  0.0351\n",
      " 0.5051 -0.3127  0.8936  0.3954 -0.6543  0.8657 -0.1654  0.9592  0.8287  1.1953\n",
      "-0.2803  0.8897  1.1810 -0.0653  0.7099  1.4811  1.4208  1.2979 -0.0694 -0.3817\n",
      " 0.5933 -0.0881 -0.1066 -0.4257  0.0075 -0.2048 -1.7848 -0.3742 -1.0483 -0.7345\n",
      "-0.6016 -1.8175  0.1243 -1.2455 -0.8114 -0.7161  0.5977 -0.6987  0.0524 -0.8667\n",
      " 1.3517 -0.0251  0.4184  0.0143  0.9916 -1.2906  0.0947  0.4591 -0.6553  0.2089\n",
      " 0.3357  0.0520 -0.3403  1.3421  0.3169 -0.1164  0.7450 -0.8583  0.3657 -0.0178\n",
      " 0.7084  0.1431  2.1606  0.0012  0.5635  0.8140  1.0198  0.1802  0.1627 -1.9319\n",
      "-0.8234 -1.0082  0.4692 -1.0969 -0.3684 -1.2028  0.6203 -1.6605  0.6156  0.6188\n",
      " 0.7899 -0.8541 -0.4620 -1.9286  0.0398 -0.5211 -0.6014 -0.2853  0.8449 -0.0928\n",
      "-0.5510 -0.0881 -0.5869  0.4249 -0.4805 -0.5812  0.2351  1.4621 -0.2184  0.5115\n",
      "-1.6913  0.2061 -0.5323 -1.2416 -1.3821  0.0059 -0.2811  0.1531  0.0302  1.9377\n",
      " 1.3923 -1.5564  1.0860 -1.1587 -0.0819  0.9275  1.7416 -1.2265 -2.1865 -0.2378\n",
      " 0.2321  0.2196 -0.6305  0.0018  1.5187  0.6556  1.2813 -0.0524  1.0704 -1.0734\n",
      " 0.9185  0.4504  0.7208 -1.3434  0.6324  0.4745 -0.7284  1.7754  0.4718  0.7148\n",
      "[torch.FloatTensor of size 49x10]\n",
      "\n",
      "1*******\n",
      "['are', 'about', 'study', 'the']\n",
      "Variable containing:\n",
      "-0.2803  0.8897  1.1809 -0.0652  0.7098  1.4810  1.4208  1.2979 -0.0694 -0.3816\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.7748 -0.7812 -0.8984  1.2845 -0.7146  2.1725 -0.9449 -0.2979 -0.0680  0.0380\n",
      "-0.4304 -2.7832  2.4551  0.0799  0.2869 -0.5868 -1.2121 -0.1456  1.1880 -2.0991\n",
      " 0.0691 -1.0045 -1.4106 -0.0915 -1.2085 -0.7630 -0.0951 -0.4810  0.6724 -0.9945\n",
      "-1.2720 -0.8173 -0.1643 -0.4783 -0.0912 -2.4625  0.9266  0.9247  0.0243 -0.3899\n",
      "-0.2930 -0.2631  1.9665  0.0819 -0.7568  0.4528  0.3981  0.8903  0.7367  0.7853\n",
      "-2.0424  0.6390 -0.5266 -1.1582 -0.1311 -0.9849  0.5880 -0.7274 -1.1458  1.2856\n",
      " 0.1971 -0.2333 -2.5347 -1.2130  0.2245 -1.5373  1.9982 -0.0735  0.2365 -0.8532\n",
      " 0.1641 -0.2455 -1.3532 -1.7252 -1.1502 -2.7381  0.7589 -0.0799  0.3698  0.0652\n",
      " 0.8204  1.4974 -1.5907  0.3252 -0.2205 -0.6457  1.4039 -0.4399 -0.7118 -0.4682\n",
      "-0.8177 -0.7242  1.1280  0.7614  0.5012  1.1098  0.1934 -2.0336 -0.7202  0.0265\n",
      " 0.8882  0.6051  2.4186 -0.7303 -1.2678 -0.1944 -1.7197  0.0886 -1.0088 -0.4802\n",
      "-0.8480  1.4590  0.2580  0.1403 -2.7081 -1.3363  0.4826 -0.2771  1.1254  0.2094\n",
      " 1.2578 -0.4973  1.5943  2.4811 -0.7333  1.2039  0.0594 -0.0303 -1.6371  0.3178\n",
      "-2.1192 -1.2335  1.5152 -0.0651  1.5387 -0.6896 -0.5029  0.5015 -0.5548  1.2587\n",
      " 0.0997  0.5919 -0.4684  0.2658  0.3243  0.3714 -0.3412  0.1161 -0.7828  0.4069\n",
      " 0.2277 -0.2330  1.9982  0.8340  0.8645  1.5306 -0.1056  0.6405  0.4192 -0.6408\n",
      " 0.5247 -0.1531  0.3436  0.9095 -0.8600  1.6047  0.2953  0.6754  0.0139 -0.3625\n",
      "-0.3361  0.4680  0.7140  1.8719 -0.9726 -0.9814 -0.3394 -1.4955  0.8482  0.9598\n",
      " 0.7000  0.5579 -1.2330  0.8950  0.5485  1.8285  0.0657 -0.9747  2.8343  1.3818\n",
      "-0.3728  0.1020  1.2786 -1.0685 -0.6933 -0.6949 -1.0272 -1.1994  1.1337  0.1489\n",
      " 0.9135  0.5590 -2.1980 -0.6292 -0.5238 -0.2812 -0.8155  0.0742 -0.3170 -0.7722\n",
      " 2.2769  0.2800  2.2228 -0.1356 -1.4328  0.5368  0.7172 -2.2966  0.5537 -0.8975\n",
      " 1.1880  0.6138 -1.0877  0.6981  0.9438 -1.6559  1.3342  1.8666 -0.0911 -1.1305\n",
      "-1.0468 -1.3789  0.2733  0.1550 -0.9395 -0.4640  0.0391  0.9944 -0.5210  0.4149\n",
      " 0.8287  1.3304  1.1711 -2.4701  0.3779 -2.0351  0.5423 -1.5328  0.0625  1.6045\n",
      "-0.7979  0.7886  1.1353 -0.2923 -1.8247 -0.2963  0.9343  0.8448  0.2216  0.5937\n",
      " 0.2049 -0.1365 -0.6240  0.3781 -0.9422  0.3294  1.1560  0.6049  0.2799 -0.5115\n",
      "-0.9022 -0.6204 -1.9109  0.8310  0.4975  0.2945 -0.4005  0.7590 -0.3010  0.9994\n",
      "-0.4798  1.3067  0.5923 -1.1248  1.2718 -1.0596 -0.4104 -0.6534 -0.8144 -1.6651\n",
      " 0.4518 -0.7686  1.0660 -0.4456  0.3354  0.7510 -0.7204 -0.0033  0.6146 -0.7342\n",
      "-0.7142  1.7092  0.1960 -0.5252 -0.0525 -2.2967 -0.4032 -0.0242  0.0237 -0.5118\n",
      "-2.3863 -0.7984  0.6167  0.9657  1.2048 -0.3058  1.3842 -0.4625 -0.2639 -1.3831\n",
      "-0.3717 -1.3293 -0.9959  0.5958  0.7258  0.9110  1.1518  0.5554  1.2097  0.5918\n",
      " 0.8163  0.6648 -1.0526  1.0582  0.1164 -1.3508  0.4547  0.2703 -1.9095  1.1432\n",
      "-1.4911  0.4841 -0.0216  1.1237  0.3548  1.3593  0.4050  0.0623 -0.3579  0.0352\n",
      " 0.5051 -0.3128  0.8937  0.3954 -0.6543  0.8657 -0.1655  0.9592  0.8286  1.1952\n",
      "-0.2803  0.8897  1.1809 -0.0652  0.7098  1.4810  1.4208  1.2979 -0.0694 -0.3816\n",
      " 0.5934 -0.0881 -0.1065 -0.4257  0.0076 -0.2048 -1.7847 -0.3742 -1.0482 -0.7345\n",
      "-0.6016 -1.8175  0.1242 -1.2455 -0.8114 -0.7161  0.5976 -0.6987  0.0524 -0.8668\n",
      " 1.3516 -0.0252  0.4185  0.0143  0.9916 -1.2905  0.0946  0.4591 -0.6552  0.2088\n",
      " 0.3357  0.0520 -0.3403  1.3420  0.3169 -0.1164  0.7449 -0.8582  0.3659 -0.0179\n",
      " 0.7084  0.1430  2.1606  0.0011  0.5635  0.8142  1.0196  0.1803  0.1628 -1.9319\n",
      "-0.8235 -1.0081  0.4691 -1.0970 -0.3684 -1.2027  0.6202 -1.6606  0.6155  0.6188\n",
      " 0.7900 -0.8541 -0.4620 -1.9286  0.0397 -0.5211 -0.6014 -0.2853  0.8449 -0.0928\n",
      "-0.5510 -0.0881 -0.5868  0.4248 -0.4806 -0.5812  0.2351  1.4621 -0.2185  0.5114\n",
      "-1.6913  0.2061 -0.5324 -1.2416 -1.3821  0.0060 -0.2811  0.1531  0.0302  1.9378\n",
      " 1.3923 -1.5564  1.0860 -1.1586 -0.0820  0.9276  1.7416 -1.2265 -2.1865 -0.2379\n",
      " 0.2322  0.2196 -0.6305  0.0018  1.5188  0.6557  1.2812 -0.0524  1.0704 -1.0733\n",
      " 0.9184  0.4504  0.7208 -1.3434  0.6324  0.4745 -0.7285  1.7754  0.4719  0.7148\n",
      "[torch.FloatTensor of size 49x10]\n",
      "\n",
      "['are', 'abstract', 'that', 'inhabit']\n",
      "Variable containing:\n",
      "-0.2802  0.8897  1.1809 -0.0651  0.7098  1.4809  1.4208  1.2979 -0.0694 -0.3816\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n",
      "Parameter containing:\n",
      " 0.7748 -0.7812 -0.8984  1.2845 -0.7146  2.1725 -0.9449 -0.2979 -0.0680  0.0380\n",
      "-0.4304 -2.7832  2.4551  0.0799  0.2869 -0.5868 -1.2121 -0.1456  1.1880 -2.0991\n",
      " 0.0691 -1.0045 -1.4106 -0.0915 -1.2085 -0.7630 -0.0951 -0.4810  0.6724 -0.9945\n",
      "-1.2720 -0.8173 -0.1643 -0.4783 -0.0912 -2.4625  0.9266  0.9247  0.0243 -0.3899\n",
      "-0.2930 -0.2631  1.9665  0.0819 -0.7568  0.4528  0.3981  0.8903  0.7367  0.7853\n",
      "-2.0424  0.6391 -0.5265 -1.1583 -0.1312 -0.9849  0.5880 -0.7274 -1.1458  1.2857\n",
      " 0.1971 -0.2333 -2.5347 -1.2130  0.2245 -1.5373  1.9982 -0.0735  0.2365 -0.8532\n",
      " 0.1641 -0.2455 -1.3532 -1.7252 -1.1502 -2.7381  0.7589 -0.0799  0.3698  0.0652\n",
      " 0.8204  1.4973 -1.5907  0.3253 -0.2206 -0.6457  1.4039 -0.4398 -0.7119 -0.4683\n",
      "-0.8177 -0.7242  1.1280  0.7614  0.5012  1.1098  0.1934 -2.0336 -0.7202  0.0265\n",
      " 0.8882  0.6051  2.4186 -0.7303 -1.2678 -0.1944 -1.7197  0.0886 -1.0088 -0.4802\n",
      "-0.8480  1.4590  0.2580  0.1403 -2.7081 -1.3363  0.4826 -0.2771  1.1254  0.2094\n",
      " 1.2577 -0.4973  1.5943  2.4812 -0.7333  1.2038  0.0592 -0.0303 -1.6371  0.3176\n",
      "-2.1192 -1.2333  1.5152 -0.0651  1.5386 -0.6894 -0.5029  0.5017 -0.5548  1.2587\n",
      " 0.0996  0.5920 -0.4685  0.2658  0.3244  0.3715 -0.3412  0.1162 -0.7829  0.4069\n",
      " 0.2277 -0.2330  1.9982  0.8340  0.8645  1.5306 -0.1056  0.6405  0.4192 -0.6408\n",
      " 0.5247 -0.1531  0.3436  0.9095 -0.8600  1.6047  0.2953  0.6754  0.0139 -0.3625\n",
      "-0.3360  0.4681  0.7140  1.8719 -0.9725 -0.9815 -0.3394 -1.4955  0.8482  0.9597\n",
      " 0.7000  0.5579 -1.2330  0.8950  0.5485  1.8285  0.0657 -0.9747  2.8343  1.3818\n",
      "-0.3728  0.1020  1.2786 -1.0685 -0.6933 -0.6951 -1.0273 -1.1994  1.1336  0.1488\n",
      " 0.9134  0.5590 -2.1979 -0.6293 -0.5238 -0.2812 -0.8156  0.0741 -0.3170 -0.7721\n",
      " 2.2769  0.2800  2.2228 -0.1356 -1.4328  0.5368  0.7172 -2.2966  0.5537 -0.8975\n",
      " 1.1880  0.6138 -1.0877  0.6981  0.9438 -1.6559  1.3342  1.8666 -0.0911 -1.1305\n",
      "-1.0468 -1.3789  0.2733  0.1550 -0.9395 -0.4640  0.0391  0.9944 -0.5210  0.4149\n",
      " 0.8287  1.3304  1.1711 -2.4701  0.3779 -2.0351  0.5423 -1.5328  0.0625  1.6045\n",
      "-0.7979  0.7885  1.1352 -0.2924 -1.8246 -0.2962  0.9342  0.8447  0.2215  0.5937\n",
      " 0.2049 -0.1365 -0.6240  0.3781 -0.9422  0.3294  1.1560  0.6049  0.2799 -0.5115\n",
      "-0.9022 -0.6204 -1.9109  0.8310  0.4975  0.2945 -0.4005  0.7590 -0.3010  0.9994\n",
      "-0.4798  1.3067  0.5923 -1.1248  1.2718 -1.0596 -0.4104 -0.6534 -0.8144 -1.6651\n",
      " 0.4519 -0.7686  1.0660 -0.4455  0.3353  0.7510 -0.7203 -0.0032  0.6145 -0.7343\n",
      "-0.7142  1.7092  0.1960 -0.5252 -0.0525 -2.2967 -0.4032 -0.0242  0.0237 -0.5118\n",
      "-2.3863 -0.7984  0.6167  0.9657  1.2048 -0.3058  1.3842 -0.4625 -0.2639 -1.3831\n",
      "-0.3717 -1.3293 -0.9959  0.5958  0.7258  0.9110  1.1518  0.5554  1.2097  0.5918\n",
      " 0.8163  0.6648 -1.0527  1.0582  0.1163 -1.3507  0.4546  0.2704 -1.9094  1.1431\n",
      "-1.4912  0.4840 -0.0217  1.1237  0.3547  1.3594  0.4050  0.0624 -0.3579  0.0352\n",
      " 0.5051 -0.3128  0.8937  0.3954 -0.6543  0.8657 -0.1655  0.9592  0.8286  1.1952\n",
      "-0.2802  0.8897  1.1809 -0.0651  0.7098  1.4809  1.4208  1.2979 -0.0694 -0.3816\n",
      " 0.5934 -0.0881 -0.1065 -0.4257  0.0076 -0.2048 -1.7847 -0.3742 -1.0482 -0.7345\n",
      "-0.6016 -1.8175  0.1242 -1.2455 -0.8114 -0.7161  0.5976 -0.6987  0.0524 -0.8668\n",
      " 1.3516 -0.0252  0.4185  0.0143  0.9916 -1.2905  0.0946  0.4591 -0.6552  0.2088\n",
      " 0.3357  0.0520 -0.3403  1.3420  0.3169 -0.1164  0.7449 -0.8582  0.3659 -0.0179\n",
      " 0.7084  0.1430  2.1606  0.0011  0.5635  0.8142  1.0196  0.1803  0.1628 -1.9319\n",
      "-0.8235 -1.0081  0.4691 -1.0970 -0.3684 -1.2027  0.6202 -1.6606  0.6155  0.6188\n",
      " 0.7900 -0.8541 -0.4620 -1.9286  0.0397 -0.5211 -0.6014 -0.2853  0.8449 -0.0928\n",
      "-0.5510 -0.0881 -0.5868  0.4248 -0.4806 -0.5812  0.2351  1.4621 -0.2185  0.5114\n",
      "-1.6914  0.2062 -0.5324 -1.2415 -1.3821  0.0061 -0.2811  0.1530  0.0301  1.9377\n",
      " 1.3923 -1.5564  1.0859 -1.1586 -0.0820  0.9274  1.7418 -1.2264 -2.1865 -0.2379\n",
      " 0.2322  0.2196 -0.6305  0.0018  1.5188  0.6557  1.2812 -0.0524  1.0704 -1.0733\n",
      " 0.9184  0.4504  0.7208 -1.3434  0.6324  0.4745 -0.7285  1.7754  0.4719  0.7148\n",
      "[torch.FloatTensor of size 49x10]\n",
      "\n",
      "[\n",
      " 228.5007\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 227.0712\n",
      "[torch.FloatTensor of size 1]\n",
      "] 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(2):\n",
    "    total_loss = torch.Tensor([0])\n",
    "    print(str(epoch)+'*******')\n",
    "    for context, target in data:\n",
    "        \n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in variables)\n",
    "        context_var = make_context_vector(context, word_to_ix)\n",
    "        if context_var.data[0] == 36:\n",
    "            print(context)\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_var)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a variable)\n",
    "        loss = loss_function(log_probs, autograd.Variable(\n",
    "            torch.LongTensor([word_to_ix[target]])))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    losses.append(total_loss)\n",
    "print(losses, len(losses))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
